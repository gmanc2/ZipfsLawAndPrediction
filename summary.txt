Consequently, we are able to apply our theory to three important types of data, all of which are inaccessible under Schwab et al.’s model: word frequencies, models where the latent variable is the sequence length, and complex datasets with high-dimensional observations.For word frequencies—the domain in which Zipf’s law was originally discovered—we show that taking the latent variable to be the part of speech (e.g. For such models—commonly called latent variable models—the unobserved (or latent) variable, z, is drawn from a distribution, P (z), and the observation, x, is drawn from a conditional distribution, P (x|z). Zipf’s law is a relation between rank order and frequency of occurrence: it states that when observations (e.g., words) are ranked by their frequency, the frequency of a particular observation is inversely proportional to its rank,

Partly because it is so unexpected, a great deal of effort has gone into explaining Zipf’s law. Empirically, these advances allowed us extend this explanation to important classes of data, including word frequencies (the first domain in which Zipf’s law was discovered), data with variable sequence length, and multi-neuron spiking activity. Finally, for models that are high dimensional and sufficiently realistic and complex that the conditional distribution, P (x|z), falls outside Schwab et al.’s model class, we show that Zipf’s law still emerges very naturally, again under mild conditions.