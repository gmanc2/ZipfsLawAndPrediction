Zipf’s law, which states that the probability of an observation is inversely proportional to its rank, has been observed in many domains. While there are models that explain Zipf’s law in each of them, those explanations are typically domain specific. Recently, methods from statistical physics were used to show that a fairly broad class of models does provide a general explanation of Zipf’s law. This explanation rests on the observation that real world data is often generated from underlying causes, known as latent variables. Those latent variables mix together multiple models that do not obey Zipf’s law, giving a model that does. Here we extend that work both theoretically and empirically. Theoretically, we provide a far simpler and more intuitive explanation of Zipf’s law, which at the same time considerably extends the class of models to which this explanation can apply. Furthermore, we also give methods for verifying whether this explanation applies to a particular dataset. Empirically, these advances allowed us extend this explanation to important classes of data, including word frequencies (the first domain in which Zipf’s law was discovered), data with variable sequence length, and multi-neuron spiking activity. Both natural and artificial systems often exhibit a surprising degree of statistical regularity. One such regularity is Zipf’s law. Originally formulated for word frequency [1], Zipf’s law has since been observed in a broad range of domains, including city size [2], firm size [3], mutual fund size [4], amino acid sequences [5], and neural activity [6, 7].

Zipf’s law is a relation between rank order and frequency of occurrence: it states that when observations (e.g., words) are ranked by their frequency, the frequency of a particular observation is inversely proportional to its rank,

Partly because it is so unexpected, a great deal of effort has gone into explaining Zipf’s law. So far, almost all explanations are either domain specific or require fine-tuning. For language, there are a variety of domain-specific models, beginning with the suggestion that Zipf’s law could be explained by imposing a balance between the effort of the listener and speaker [8–10]. Other explanations include minimizing the number of letters (or phonemes) necessary to communicate a message [11], or by considering the generation of random words [12]. There are also domain-specific models for the distribution of city and firm sizes. These models propose a process in which cities or firms grow by random amounts [2, 3, 13], with a fixed total population or wealth and a fixed minimum size. Other explanations of Zipf’s law require fine tuning. For instance, there are many mechanisms that can generate power laws [14], and these can be fine tuned to give an exponent of −1. Possibly the most important fine-tuned proposal is the notion that some systems sit at a highly unusual thermodynamic state—a critical point [6, 15–18].Only very recently has there been an explanation, by Schwab and colleagues [19], that does not require fine tuning. This explanation exploits the fact that most real-world datasets have hidden structure that can be described using an unobserved variable. For such models—commonly called latent variable models—the unobserved (or latent) variable, z, is drawn from a distribution, P (z), and the observation, x, is drawn from a conditional distribution, P (x|z). The distribution over x is therefore given byFor example, for neural data the latent variable could be the underlying firing rate or the time since stimulus onset.

While Schwab et al.’s result was a major advance, it came with some restrictions: the observations, x, had to be a high dimensional vector, and the conditional distribution, P (x|z), had to lie in the exponential family with a small number of natural parameters. In addition, the result relied on nontrivial concepts from statistical physics, making it difficult to gain intuition into why latent variable models generally lead to Zipf’s law, and, just as importantly, why they sometimes do not. Here we use the same starting point as Schwab et al. (Eq 2), but take a very different theoretical approach—one that considerably extends our theoretical and empirical understanding of the relationship between latent variable models and Zipf’s law. This approach not only gives additional insight into the underlying mechanism by which Zipf’s law emerges, but also gives insight into where and how that mechanism breaks down. Moreover, our theoretical approach relaxes the restrictions inherent in Schwab et al.’s model [19] (high dimensional observations and an exponential family distribution with a small number of natural parameters). Consequently, we are able to apply our theory to three important types of data, all of which are inaccessible under Schwab et al.’s model: word frequencies, models where the latent variable is the sequence length, and complex datasets with high-dimensional observations.For word frequencies—the domain in which Zipf’s law was originally discovered—we show that taking the latent variable to be the part of speech (e.g. noun/verb) can explain Zipf’s law. As part of this explanation, we show that if we take only one part of speech (e.g. only nouns) then Zipf’s law does not emerge—a phenomenon that is not, to our knowledge, taken into account by any other explanation of Zipf’s law for words. For models in which the latent variable is sequence length (i.e. observations in which the dimension of the vector, x, is variable), we show that Zipf’s law emerges under very mild conditions. Finally, for models that are high dimensional and sufficiently realistic and complex that the conditional distribution, P (x|z), falls outside Schwab et al.’s model class, we show that Zipf’s law still emerges very naturally, again under mild conditions. In addition, we introduce a quantity that allows us to assess how much a given latent variable contributes to the observation of Zipf’s law in a particular dataset. This is important because it allows us to determine, quantitatively, whether a particular latent variable really does contribute significantly to Zipf’s law.